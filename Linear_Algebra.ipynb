{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVZAxH0gKZMQbE7vu6WBT5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MMBAYAR/BMAT24/blob/main/Linear_Algebra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BUAD122 - Business Mathematics$^1$**\n",
        "----\n",
        "### **Week1:<br>Linear Algebra Basics**<br>\n",
        "This session will introduce basic linear algebra terminology and deliver mechanisms for performing matrix operations.\n",
        "<br><br><br><br>\n",
        "### ***Scalars, Vectors, Matrices, and Arrays of Higher Dimensions***<br>\n",
        "- **Scalar:** A scalar is a dimensionless (0-D) quantity.<br>*Some examples:*<br>\n",
        "$\\begin{array}{lll} \\begin{bmatrix}-2\\end{bmatrix} &,& \\begin{bmatrix}\\pi\\end{bmatrix} &,& \\begin{bmatrix}2.71\\end{bmatrix} \\end{array} $\n",
        "<br>Also, scalars can be written without the brackets:<br>\n",
        "$\\begin{array}{lll}\\sqrt{-{2 \\over 3}}&,&e&,&3.14\\end{array}$\n",
        "<br>A scalar simply is a number...<br><br>\n",
        "- **Vector:** A vector is a one-dimensional (1-D) array of quantities.\n",
        " - Vectors both have a magnitude and a direction.<br>*Some examples:*<br>\n",
        "   - A column vector: $\\vec u = \\begin{bmatrix}-2\\\\\\pi\\\\2.71\\end{bmatrix}$\n",
        "   - A row vector: ${\\vec v}^T = \\begin{bmatrix}\\sqrt{-{2 \\over 3}},&e,&3.14\\end{bmatrix}^T$\n",
        " - Vectors are denoted as bold-face lowercase letters with a rightwards arrow above them. <br>E.g. $\\vec x$\n",
        " - The default form of a vector is a column vector. Therefore row vectors are denoted as ***transposed vectors***.\n",
        " - The vector $\\vec x= \\begin{bmatrix}3\\\\1\\\\2\\end{bmatrix}$ has 3 elements:\n",
        "   - the first element is 3,\n",
        "   - the second element is 1,<br>and\n",
        "   - the third element is 2\n",
        " - To state the coordinates of a point in an n-dimensional space (a value composition of n variables), it is common to use a column vector with n elements. <br><br>\n",
        "- **Matrix:** A matrix is a two-dimensional array of quantities.\n",
        " - Matrices can be perceived as stacked row vectors.<br>An example:<br>\n",
        " $A=\\begin{bmatrix}3&1&2\\\\e^3&\\sqrt{-1}&0\\end{bmatrix}$<br>A is a 2x3 (two by three) matrix.<br><br>\n",
        "- **Tensor:** Tensors are general names for multi-dimensional arrays of quantitites.\n",
        " - A scalar is a tensor of rank 0,\n",
        " - A vector is a tensor of rank 1,\n",
        " - A matrix is a tensor of rank 2.<br>\n",
        "\n",
        " Tensors of higher rank do not have special names...\n",
        "<br><br>**This maybe helpful:**<br>\n",
        "Consider;\n",
        "- scalars as *numbers*,\n",
        "- vectors as *lists of numbers*,\n",
        "- matrices as *tables of numbers*,<br><br><br><br><br>\n",
        "\n",
        "### ***Matrix Operations***\n",
        "\n",
        "A vector (a column vector) is a \"*something*\"-by-one matrix and a row vector is a one-by-\"*something*\" vector:<br>\n",
        "E.g.:<br>\n",
        "$X=\\begin{bmatrix}3\\\\1\\\\2\\end{bmatrix}$<br>*X* is a 3 by 1 matrix (a columns vector).<br><br>$Y=\\begin{bmatrix}3,&1,&2\\end{bmatrix}$<br>*Y* is a 1 by 3 matrix (a row vector).<br><br>\n",
        "$A=\\begin{bmatrix}3 & 5 & 8\\\\0  & -1 & 2\\end{bmatrix}$<br>*A* is a 2 by 3 matrix.<br><br><br>\n",
        "Elements of matrix *A* is generally of the form \"$a_{ij}$\" where, *i* indicates on which row that element is, and *j* indicates in which column that element is. For instance, for the matrix *A*, here are some elements with their corresponding values:\n",
        " - $a_{23}=2$,\n",
        " - $a_{11}=3$,\n",
        " - $a_{21}=0$<br><br><br><br><br>\n",
        "\n",
        "#### **Vector Operations**\n",
        "* **Addition of vectors...**\n",
        "\n",
        "First of all, before summing two or more vectors up, one must check if their shapes are identical or not.<br>\n",
        "$\\vec x = \\begin{bmatrix}\\pi\\\\-1\\\\0\\end{bmatrix}$ is an 3x1 matrix<br><br>\n",
        "$\\vec y = \\begin{bmatrix}0,&-e,&1\\end{bmatrix} $ is an 1x3 matrix<br><br>\n",
        "$\\vec z = \\begin{bmatrix}cos{({\\pi \\over 2})}\\\\i\\end{bmatrix}$ is a 2x1 matrix<br><br><br>\n",
        "x,y, and z have different shapes...<br>\n",
        "***Only vectors with identical shapes can be added.***<br>\n",
        "\n",
        "we may not sum $\\vec x$ and $\\vec y$ but $\\vec y^T$ is 3x1... So, may be we can add $\\vec y^T$ to $\\vec x$:<br>\n",
        "$\\vec x + \\vec y^T = \\begin{bmatrix}x_{11} + y^T_{11}\\\\x_{21} + y^T_{21}\\\\x_{31} + y^T_{31}\\end{bmatrix} = \\begin{bmatrix}\\pi + 0\\\\-1 + (-e)\\\\0+1\\end{bmatrix} = \\begin{bmatrix}\\pi\\\\-1-e\\\\1\\end{bmatrix}$<br><br><br><br><br>\n",
        "\n",
        "* **Subtracting a vector from another vector...**\n",
        "\n",
        "Subtracting a vector $\\vec v$ from a vector $\\vec u$ is simply adding $-1$ times $\\vec v$ to vector $\\vec u$:<br><br>\n",
        "$\\vec u=\\begin{bmatrix}3\\\\1\\\\2\\end{bmatrix}$, $\\vec v=\\begin{bmatrix}\\sqrt{2}\\\\0\\\\\\log_e{21}\\end{bmatrix}$<br><br>\n",
        "$-1\\cdot \\vec v=-\\vec v=\\begin{bmatrix}-\\sqrt{2}\\\\0\\\\-\\log_e{21}\\end{bmatrix}$<br><br>\n",
        "$\\vec u-\\vec v=\\vec u+(-1)\\cdot \\vec v=\\begin{bmatrix}u_{11}+(-1)\\cdot v_{11}\\\\u_{21}+(-1)\\cdot v_{21}\\\\u_{31}+(-1)\\cdot v_{31}\\end{bmatrix}=\\begin{bmatrix}3-\\sqrt{2}\\\\1-0\\\\2-\\log_e{21}\\end{bmatrix}=\\begin{bmatrix}3-\\sqrt{2}\\\\1\\\\2-\\log_e{21}\\end{bmatrix}$<br><br><br><br><br>\n",
        "\n",
        "* **Hadamard product of vectors (element-wise multiplication)...**\n",
        "\n",
        "Matrices (or vectors) with identical shape can be used in a Hadamard product operation. Hadamard product or element-wise multiplication gives a new matrix that has the same shape with those used in the element-wise multiplication operation. This ouput matrix's elements are products of input matrices' elements on the same row and in the same column.<br><br>\n",
        "$\\vec u=\\begin{bmatrix}3\\\\1\\\\2\\end{bmatrix}$, $\\vec v=\\begin{bmatrix}\\sqrt{2}\\\\0\\\\\\log_e{21}\\end{bmatrix}$<br><br>\n",
        "$\\vec u \\odot \\vec v = \\begin{bmatrix}u_{11}v_{11}\\\\u_{21}v_{21}\\\\u_{31}v_{31}\\end{bmatrix} = \\begin{bmatrix}3\\sqrt{2}\\\\1 \\cdot 0\\\\2\\log_e{21}\\end{bmatrix}= \\begin{bmatrix}3\\sqrt{2}\\\\0\\\\2\\log_e{21}\\end{bmatrix}$<br><br>\n",
        "The Hadamard product comes in handy while explaining the *partial derivatives* and the *directional derivatives* for functions of several variables (multi-variable calculus).<br><br><br><br><br>\n",
        "\n",
        "* **Dot product of vectors (matrix multiplication)...**\n",
        "\n",
        "For two matrices such that the number of columns in the first matrix is identical to the number of rows in the second matrix, the dot product produces a matrix that has the same number of with the first input matrix and the same number of columns with the second input matrix. And, each element of the output matrix on the $i^{th}$ row and $j^{th}$ column is sum-product of the $i^{th}$ row elements of the $1^{st}$ input matrix with the $j^{th}$ column elements of the $2^{nd}$ input matrix:<br><br>\n",
        "- Let's dot product a row vector with a column vector;<br>$\\vec x = \\begin{bmatrix}3\\\\1\\\\2\\end{bmatrix}$, $\\vec y = \\begin{bmatrix}{2 \\over 3}\\\\-1\\\\\\sqrt{7}\\end{bmatrix}$<br><br>$\\vec x^T \\cdot \\vec y= \\begin{bmatrix}3,&1,&2\\end{bmatrix} \\cdot \\begin{bmatrix}{2 \\over 3}\\\\-1\\\\\\sqrt{7}\\end{bmatrix}=x_{11}^Ty_{11}+x_{12}^Ty_{21}+x_{13}^Ty_{31}=3({2 \\over 3})+(1)(-1)+2\\sqrt{7}=1+2\\sqrt{7}=c_{11}$<br><br>\n",
        "The sum product of the first row in $\\vec x^T$ and the first column in $\\vec y$ gives the output matrix's element on the first row and first column ($c_{11}$). <br>The dot product of $\\vec x^T$ and $\\vec y$ is a 1x1 matrix (a scalar).<br><br>\n",
        "- Let's dot product a column vector with a row vector;<br>\n",
        "$\\vec x \\cdot \\vec y^T=\\begin{bmatrix}3\\\\1\\\\2\\end{bmatrix} \\cdot \\begin{bmatrix}{2 \\over 3},&-1,&\\sqrt{7}\\end{bmatrix}=\n",
        "\\begin{bmatrix}x_{11}y^T_{11}&x_{11}y^T_{21}&x_{11}y^T_{31}\\\\x_{12}y^T_{11}&x_{12}y^T_{21}&x_{12}y^T_{31}\\\\x_{13}y^T_{11}&x_{13}y^T_{21}&x_{13}y^T_{31}\\end{bmatrix}$\n",
        "<br><br>\n",
        "$=\\begin{bmatrix}2&-3&3\\sqrt{7}\\\\{2 \\over 3}&-1&\\sqrt{7}\\\\{4 \\over 3}&-2&2\\sqrt{7}\\end{bmatrix}=\n",
        "\\begin{bmatrix}c_{11}&c_{12}&c_{13}\\\\c_{21}&c_{22}&c_{23}\\\\c_{31}&c_{32}&c_{33}\\end{bmatrix}=C$<br><br>\n",
        "This time the output matrix $C$ is a 3x3 matrix. $C$ indeed is a peculiar matrix, please notice that each row is a multiple of the $\\vec y^T$ vector. Next session we will discuss what is wrong with matrix $C$ formally.<br><br><br><br><br>\n",
        "\n",
        "* **Length of a vector (Euclidean norm) and vector normalization**\n",
        "\n",
        "Length of a vector, is the distance of the point that the vector represents in the n-dimensional space from a reference point ($\\vec 0$ unless told otherwise):<br><br>\n",
        " - The L-k norm of a vector:<br>\n",
        "$||\\vec x||=\\sqrt[k]{\\sum^n_{i=1}{x_i^k}}$\n",
        " - The L-1 norm of a vector (Manhattan distance):<br>\n",
        "$||\\vec x||=x_1 + \\dots + x_n$\n",
        " - The L-2 norm of a vector (Euclidean norm):<br>\n",
        "$||\\vec x||=\\sqrt{\\sum^n_{i=1}{x_i^2}}$\n",
        "\n",
        "The Euclidean norm is the most common distance metric.<br>\n",
        "Dividing a vector by its norm is called normalization. Here is the formula to compute a normalized version of the vector $\\vec  x$:<br><br>\n",
        "$\\bar x=\\left( {1\\over {||\\vec x||}} \\right) \\cdot \\vec x$<br><br>\n",
        "A normalized vector's length is 1. We will use normalized vectors during our talk on the directional derivatives.<br><br><br><br><br>\n",
        "\n",
        "----\n",
        "\n",
        "$(^1):$ Scope of this course is<br>\n",
        "- Mathematics for decision-making,\n",
        "- A gentle introduction to mathematics for data science<br><br><br><br>\n"
      ],
      "metadata": {
        "id": "OsSXSFkOGmiD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Week2:<br>Matrix Operations, Length & Determinants, Inverse Matrices**<br>\n",
        "This session will continue to introduce basic linear algebra terminology and deliver mechanisms for performing matrix operations.\n",
        "<br><br><br><br>\n",
        "### ***Matrix Operations (cont'd)***<br>\n",
        "Last week we discussed that matrices were rectengular (two-dimensional) arrays of numbers. As long as shape requirements are satisfied we can also operate on matrices. By the way we already did, vectors are matrices...<br><br>\n",
        "* **Addition & Subtraction**<br>\n",
        "We can add matrices as long as their shapes are identical:<br>\n",
        "$A=\\begin{bmatrix}1&2\\\\3&4\\\\5&6\\end{bmatrix}$,\n",
        "$B=\\begin{bmatrix}1&3&5\\\\2&4&6\\end{bmatrix}$, and\n",
        "$C=\\begin{bmatrix}7&5\\\\24&12\\\\25&13\\end{bmatrix}$<br><br>\n",
        "$A$ and $C$ have identical shapes but shape of $B$ does not match the other two...<br>\n",
        " - We may add $A$ and $C$ together,\n",
        " - We can not add $B$ to $A$ or $C$,\n",
        " - But, we may add $A$, $B^T$ and $C$ together<br><br>\n",
        "$A+B^T+C=\\begin{bmatrix}a_{11}+b^T_{11}+c_{11}&a_{12}+b^T_{12}+c_{12}\\\\a_{21}+b^T_{21}+c_{21}&a_{22}+b^T_{22}+c_{22}\\\\a_{31}+b^T_{31}+c_{31}&a_{32}+b^T_{32}+c_{32}\\end{bmatrix}\n",
        "=\\begin{bmatrix}1+1+7&2+2+5\\\\3+3+24&4+4+12\\\\5+5+25&6+6+13\\end{bmatrix}=\\begin{bmatrix}9&9\\\\30&20\\\\35&25\\end{bmatrix}$\n",
        "\n",
        " - Actually, $B^T=A$<br><br>\n",
        "\n",
        "* **The principal diagoanal and the transpose of a matrix**<br>\n",
        "Let's use the most famous diagonal matrix, the unit matrix to show what a principal diagonal is:<br><br>\n",
        "$I = \\begin{bmatrix}\n",
        "1&0&\\dots&0\\\\\n",
        "0&1&\\dots&0\\\\\n",
        "\\vdots&\\vdots&\\ddots&\\vdots\\\\\n",
        "0&0&\\dots&1\\\\\n",
        "\\end{bmatrix}$<br><br>\n",
        "Please also notice that this unit matrix ($I$) is a square matrix. And all the non-zero values are on the **principal diagonal** (from top-left to bottom-right).<br><br>\n",
        "The transposing of a matrix is just swapping all the elements' indices. In other words transposing is fliping a matrix accross the principal diagonal.\n",
        " - *Transpose of a square matrix:*<br>$A$ is a 3x3 matrix;<br>\n",
        " $A=\\begin{bmatrix}\n",
        " a_{11}&a_{12}&a_{13}\\\\\n",
        " a_{21}&a_{22}&a_{23}\\\\\n",
        " a_{31}&a_{32}&a_{33}\\\\\n",
        " \\end{bmatrix}=\\begin{bmatrix}\n",
        " 3&1&2\\\\\n",
        " \\sqrt2&-1&0\\\\\n",
        " {1\\over 3}&7&\\pi\n",
        " \\end{bmatrix}⇒A^T=\\begin{bmatrix}\n",
        " a_{11}&a_{21}&a_{31}\\\\\n",
        " a_{12}&a_{22}&a_{32}\\\\\n",
        " a_{13}&a_{23}&a_{33}\\\\\n",
        " \\end{bmatrix}=\\begin{bmatrix}\n",
        " 3&\\sqrt2&{1\\over 3}\\\\\n",
        " 1&-1&7\\\\\n",
        " 2&0&\\pi\n",
        " \\end{bmatrix}$<br><br>\n",
        " - *Transpose of a rectengular matrix:*<br>$B$ is a 2x3 matrix;<br>\n",
        " $B=\\begin{bmatrix}\n",
        " b_{11}&b_{12}&b_{13}\\\\\n",
        " b_{21}&b_{22}&b_{23}\\\\\n",
        " \\end{bmatrix}=\\begin{bmatrix}\n",
        " 3&1&2\\\\\n",
        " \\sqrt2&-1&0\n",
        " \\end{bmatrix}⇒B^T=\\begin{bmatrix}\n",
        " b_{11}&b_{21}\\\\\n",
        " b_{12}&b_{22}\\\\\n",
        " b_{13}&b_{23}\n",
        " \\end{bmatrix}=\\begin{bmatrix}\n",
        " 3&\\sqrt2\\\\\n",
        " 1&-1\\\\\n",
        " 2&0\n",
        " \\end{bmatrix}$\n",
        " <br><br>\n",
        "\n",
        "* **Matrix multiplication (dot product of matrices)**<br>\n",
        " - **Linear transformation of a point in 2-dimensional space**<br>\n",
        " This is just *vector-matrix multiplication*.<br>\n",
        " To multiply a vector $\\vec x$ with two elements (a point in 2-dimensional space) by a matrix $A$, one has to write down \" $A\\cdot \\vec x$ \":<br><br>\n",
        " $A=\\begin{bmatrix}a_{11}&a_{12}\\\\a_{21}&a_{22}\\end{bmatrix}=\\begin{bmatrix}-1&3\\\\2&1\\end{bmatrix}$,<br><br>$\\vec x=\\begin{bmatrix}x_{1}\\\\x_{2}\\end{bmatrix}=\\begin{bmatrix}\\pi\\\\e\\end{bmatrix}$<br><br>\n",
        " $A\\cdot \\vec x = \\begin{bmatrix}-1&3\\\\2&1\\end{bmatrix} \\cdot \\begin{bmatrix}\\pi\\\\e\\end{bmatrix} = \\begin{bmatrix}(-1)\\pi + (3)e\\\\(2)\\pi+(1)e\\end{bmatrix} = \\begin{bmatrix}3e-\\pi\\\\2\\pi+e\\end{bmatrix}$<br><br>\n",
        " Please recall that order matters in matrix multiplication, and when we multiply a linear algebra by another, the multiplier is written to the left-hand-side!<br>\n",
        " Please notice that the horizontal basis vecor $\\vec i = \\begin{bmatrix}1\\\\0\\end{bmatrix}$ has moved to $\\hat i = \\begin{bmatrix}-1\\\\2\\end{bmatrix}$ and the vertical basis vecor $\\vec j = \\begin{bmatrix}0\\\\1\\end{bmatrix}$ has moved to $\\hat j = \\begin{bmatrix}3\\\\1\\end{bmatrix}$<br>Therefore the area defined by the basis vectors is $\\hat i \\cdot \\hat j = 7$ (actually negative $7$) now instead of $\\vec i \\cdot \\vec j = 1$.<br>Soon we'll talk about what does this imply on about the matrix $A$.<br><br>\n",
        "\n",
        "\n",
        "* **The principal minors of a square matrix**<br>\n",
        "A minor ($M_n$) of the matrix $A$, is a submatrix of $A$ such that the deleted rows and columns of $A$ have the same indices.<rb>\n",
        "Imagine a 3x3 matrix:<br>\n",
        "$A=\\begin{bmatrix}\n",
        " a_{11}&a_{12}&a_{13}\\\\\n",
        " a_{21}&a_{22}&a_{23}\\\\\n",
        " a_{31}&a_{32}&a_{33}\\\\\n",
        " \\end{bmatrix}=\\begin{bmatrix}\n",
        " 3&\\sqrt2&{1\\over 3}\\\\\n",
        " 1&-1&7\\\\\n",
        " 2&0&\\pi\n",
        " \\end{bmatrix}$<br><br>\n",
        " Principal minors of A:\n",
        "  - $M_1 = \\begin{bmatrix}a_{11}\\end{bmatrix}= \\begin{bmatrix}3\\end{bmatrix}$<br><br>\n",
        "  - $M_2 = \\begin{bmatrix}a_{11}&a_{12}\\\\a_{21}&a_{22}\\end{bmatrix} = \\begin{bmatrix}3&\\sqrt2\\\\1&-1\\end{bmatrix}$<br><br>\n",
        "  - $M_3=\\begin{bmatrix}\n",
        " a_{11}&a_{12}&a_{13}\\\\\n",
        " a_{21}&a_{22}&a_{23}\\\\\n",
        " a_{31}&a_{32}&a_{33}\\\\\n",
        " \\end{bmatrix} = \\begin{bmatrix}\n",
        " 3&\\sqrt2&{1\\over 3}\\\\\n",
        " 1&-1&7\\\\\n",
        " 2&0&\\pi\n",
        " \\end{bmatrix}$\n",
        "\n",
        " <br>Observe that the principal minors always include the first element and expand along the principal diagonal<br><br>\n",
        "\n",
        "* **Determninat, and rank of a matrix**<br>\n",
        "[Determininat is how many times (and in what direction) the unit square area formed by the basis vectors have shrank or grew after being linearly transformed by a matrix.](https://www.3blue1brown.com/lessons/determinant)<br>\n",
        "Determinant can be interpreted as the \"*directional volume*\" of a matrix. If the determeninat of a matrix is *non-zero*, then all rows and all columns of that matrix are linearly independent.<br>\n",
        "Recall the dot product of a column vector by a row vector whilst the vector operations session...<br><br>\n",
        "$\\vec x \\cdot \\vec y^T=\\begin{bmatrix}3\\\\1\\\\2\\end{bmatrix} \\cdot \\begin{bmatrix}{2 \\over 3},&-1,&\\sqrt{7}\\end{bmatrix}=\n",
        "\\begin{bmatrix}x_{11}y^T_{11}&x_{11}y^T_{21}&x_{11}y^T_{31}\\\\x_{12}y^T_{11}&x_{12}y^T_{21}&x_{12}y^T_{31}\\\\x_{13}y^T_{11}&x_{13}y^T_{21}&x_{13}y^T_{31}\\end{bmatrix}$\n",
        "<br><br>\n",
        "$=\\begin{bmatrix}2&-3&3\\sqrt{7}\\\\{2 \\over 3}&-1&\\sqrt{7}\\\\{4 \\over 3}&-2&2\\sqrt{7}\\end{bmatrix}=\n",
        "\\begin{bmatrix}c_{11}&c_{12}&c_{13}\\\\c_{21}&c_{22}&c_{23}\\\\c_{31}&c_{32}&c_{33}\\end{bmatrix}=C$<br><br>\n",
        "$C$ is a 3x3 square matrix. There is something wrong with the matrix $C$, the rows of this matrix are all linearly dependent. That is, you can write some other row as $k\\cdot \\vec y + t$ (some affine transformation of $\\vec y$) where $k,t$ are some constat numbers. <br><br>\n",
        "For any matrix, if the a column or a row can be written as an affine transformation of some other column or some other row then the matrix is not a full rank matrix and its determinant is 0.<br><br>\n",
        "For a 2x2 square matrix, here is how to compute the determinant:<br><br>\n",
        " - $A = \\begin{bmatrix}a_{11}&a_{12}\\\\a_{21}&a_{22}\\end{bmatrix} = \\begin{bmatrix}3&4\\\\-1&2\\end{bmatrix}$<br><br>\n",
        " - $det A=|A|=a_{11}a_{22}-a_{12}a_{21}$<br><br>\n",
        " - $|A| = 6 - (-4) = 6 + 4 = 10$<br><br>\n",
        "Apperantly, $A$ is a full rank square matrix. Its rank is smallest of the number of linearly independent columns and the number of linearly independent columns (should be the same for any square matrix). For this instance rank of $A$ is $2$.<br><br>\n",
        "\n",
        "* **Invertibility, and inverse matrix**<br>\n",
        "Any full-rank square matrix is invertible. That is, if a matrix has;\n",
        " - an equal number of rows and columns,\n",
        " - linearly independent rows on all of its rows,\n",
        " - linearly independent columns in all of its columns\n",
        "\n",
        " , then the matrix is said to be non-singular (invertible).<br>\n",
        " Any square matrix that is not invertible is called a **singular** matrix. To test whether a square matrix is a full rank, one must compute its determinant. If the determinant is non-zero, then the matrix is full rank.<br>\n",
        "Generally, we are found of invertible matrices...<br><br><br><br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z3DPhEiX7_El"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Week3:<br>Systems of Linear Equations**<br>\n",
        "In this session we will cover a procedure for solving systems of linear equations.\n",
        "<br><br><br><br>\n",
        "### ***Linear Equations and Linear Inequalities***<br>\n",
        "To form an equality or an inequality we have to compare to mathematical objects (quantities or functions).<br>If we are comparing an affine function or a linear function with a quantity then we have a linear equation or a linear inequality.<br><br>\n",
        "Later on we will discuss what is the difference between an affine function and a linear function and for simplicity we will only mention linear functions here after.<br><br>\n",
        "- If a linear function of several variables $\\Big(x_1,\\dots,x_n \\rightarrow g_i(x_1,\\dots,x_n)\\Big)$ is less than a constant quantity (say, $b_i$) then, we have<br>$g_i(x_1,\\dots,x_n)=g_i(\\vec x)<b_i$\n",
        "- If a linear function of several variables $\\Big(x_1,\\dots,x_n \\rightarrow g_i(x_1,\\dots,x_n)\\Big)$ is less than or equal to a constant quantity (say, $b_i$) then, we have<br>$g_i(x_1,\\dots,x_n)=g_i(\\vec x)\\leq b_i$\n",
        "- If a linear function of several variables $\\Big(x_1,\\dots,x_n \\rightarrow g_i(x_1,\\dots,x_n)\\Big)$ is equal to a constant quantity (say, $b_i$) then, we have<br>$g_i(x_1,\\dots,x_n)=g_i(\\vec x)= b_i$\n",
        "- If a linear function of several variables $\\Big(x_1,\\dots,x_n \\rightarrow g_i(x_1,\\dots,x_n)\\Big)$ is greater than or equal to a constant quantity (say, $b_i$) then, we have<br>$g_i(x_1,\\dots,x_n)=g_i(\\vec x)\\geq b_i$\n",
        "- If a linear function of several variables $\\Big(x_1,\\dots,x_n \\rightarrow g_i(x_1,\\dots,x_n)\\Big)$ is greater than a constant quantity (say, $b_i$) then, we have<br>$g_i(x_1,\\dots,x_n)=g_i(\\vec x)> b_i$\n",
        "<br><br>\n",
        "\n",
        "If the equality condition is not allowed in such a comparison, then we have a ***strict inequality*** ($g_i(\\vec x)<b_i$ or $g_i(\\vec x)>b_i$). If we have an inequality where the equality condition is also allowed, then we have a ***soft inequality***. And, finally, if we only allow the equality condition, then we have an ***equality***.<br><br>\n",
        "It is easy to swap an inequality. If both sides of an inequality were multiplied by a negative number, say $-1$, the inequality would be swapped:<br><br>\n",
        " - Let the inequality to be swapped be<br>$3x_1-2x_2 \\leq 21$\n",
        " - If both sides were multiplied by $-1$; the left hand side would be $-3x_1+2x_2$, the right hand side would be $-21$ and the inequality would become<br>$-3x_1+2x_2 \\geq -21$<br>Please notice that the sense of the inequality is also swapped.<br><br>\n",
        "\n",
        "Swapping an equality is very similar...\n",
        " - Let our equality be<br>$x_1+3x_2-0.5x_3 = -5$\n",
        " - If both sides of this equality were multiplied by $-1$; the left hand side would be $-x_1-3x_2+0.5x_3$, the right hand side would be $5$, and the equality would become<br>$-x_1-3x_2+0.5x_3 = 5$<br><br>\n",
        "\n",
        "Another trick we can apply to a soft inequality is transforming it into an equality by introducing an ***auxilliary variable***.<br>\n",
        "Just imagine a scale...<br>\n",
        "If you have a less tha or equal to sense (\"$\\leq$\"), the right hand side would be at least as heavy as the left hand side on the scale. To balance the scale, you may add some weight to the left hand side.<br>\n",
        "Soft inequalities shade a half-space. Thus, is they have a non-empyt solution set, there are infinitely many solutions that correspond to different values of the auxilliary variable that transforms each state of the inequality to an equality. This is why we do not specify any value and employ a \"variable\".<br>\n",
        " - If our soft inequality is<br>$3x_1-2x_2 \\leq 21$\n",
        " - We may add an auxilliary variable \"$s_i$\" to the left hand side<br>$3x_1-2x_2+s_i = 21$<br><br>\n",
        "\n",
        "We may have several inequality and we may want to keep track of the auxilliary variables by indexing them accordingly. And, traditionally the notation for these types of auxilliary variables is \"$s$\" for \"***slack variable***\".<br>\n",
        "\n",
        "Similarly, we may also transform an inequality with the greater than or equal to sense (\"$\\geq$\"). This time the left hand side of the scale is at least as heavy as the right hand side and to maintain balance we shall add some weight to the right hand side of the scale:\n",
        " - If our soft inequality is<br>$3x_1-2x_2 \\geq 21$\n",
        " - We may add an auxilliary variable \"$s_i$\" to the right hand side<br>$3x_1-2x_2 = 21+s_i$\n",
        " - For the sake of simplicity let us move all variables to the left hand side and keep all constants on the right hand side:<br>$3x_1-2x_2-s_i = 21$<br><br>\n",
        "\n",
        "Please notice that adding a variable to the right hand side is the same as subtracting a variable from the left hand side.<br><br>\n",
        "\n",
        "* **Systems of Linear Equations**<br>\n",
        "\n",
        "Now, we are able to transform soft inequalities into equalities. Assuming we only have several equalities, we have a \"***system of equations***\". And ıf all left hand side functions are linear we would have a \"***system of linear equations***\".<br><br>\n",
        "For solving systems of linear equations we have 3 important properties:\n",
        "1. If the number of equations is the same as the number of variables, we would either have a single solution or no solution at all.\n",
        "2. If the number of equations is less than the number of variables, we would have either infinitely many solutions or no solution at all.\n",
        "3. If the number of equations is more than the number of variables, we would have no solution.<br>\n",
        "\n",
        "Having a single solution to a system of linear equations makes things simple.<br>Think of the equations as limitations to a decision. Having a single solution means you have only one option. It is clear what you would decide. On the other hand, you don't have any flexibility or freedom.<br><br>\n",
        "\n",
        "Having multiple solutions to a system of linear equations imply some flexibility and freedom. But, this time you have to choose among many solutions.<br>Which option is the best?<br>You may need some criteria to decide whether a solution is good or inferior.<br><br>\n",
        "\n",
        "If you do not have any solution to a system of linear equations because you just have too many limitations (equations), you may want to soften some strict rules or re-consider violation of some constraints.<br><br>\n",
        "\n",
        "* **A Solution Method for Solving Systems of Linear Equations**<br>\n",
        "\n",
        "Invertability and inverse matrices were covered on week 2. Now, we'll build on this knowledge.<br><br>\n",
        "For a system of linear equations with 3 variables and 3 equations:<br>\n",
        "$\\begin{matrix}\n",
        "3x_1&+&1x_2&+&2x_3&=&12\\\\\n",
        "2x_1&-&2x_2&-&2x_3&=&-22\\\\\n",
        "3x_1&-&4x_2&+&6x_3&=&46\n",
        "\\end{matrix}$<br><br>\n",
        "Can be re-written in ***matrix notation***:<br>\n",
        "- Given<br>\n",
        "$A=\\begin{bmatrix}3&1&2\\\\2&-2&-2\\\\3&-4&6\\end{bmatrix}$, $\\vec x=\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}$, $\\vec b=\\begin{bmatrix}12\\\\-22\\\\46\\end{bmatrix}$\n",
        "- The system in matrix notation is;<br>\n",
        "$A \\cdot \\vec x = \\vec b$<br><br>\n",
        "$\\begin{bmatrix}3&1&2\\\\2&-2&-2\\\\3&-4&6\\end{bmatrix} \\cdot \\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}=\\begin{bmatrix}12\\\\-22\\\\46\\end{bmatrix}$\n",
        "\n",
        "Since this system has the same number of equations and variables, there can either be a single solution or no solution at all.<br><br>\n",
        "\n",
        "To find this solution (assuming it exists), we may solve for x:<br>\n",
        "$\\vec x = A^{-1} \\cdot \\vec b$<br><br>\n",
        "$\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}=\\begin{bmatrix}3&1&2\\\\2&-2&-2\\\\3&-4&6\\end{bmatrix}^{-1} \\cdot \\begin{bmatrix}12\\\\-22\\\\46\\end{bmatrix} \\approx \\begin{bmatrix}-1.951\\\\0.244\\\\8.805\\end{bmatrix}$<br><br>\n",
        "\n",
        "Please notice, this method can only be used when $A$ is invertible. That is, $A$ must be a full-rank square matrix. In other words, the system of linear equations must have the same number of variables and equations.<br>\n",
        "- If $A$ is not full-rank, the equation that correspond to one of the linearly dependent rows may be dropped or the variable that correspond to one of the linearly dependent columns may be dropped.\n",
        "- For more information on invertibility, please check ***pseudo-inverse of a matrix*** out.\n",
        "- For more solutions methods you may want to do a small research on ***the simplex algorithm***.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H8Vx7s-zvGec"
      }
    }
  ]
}